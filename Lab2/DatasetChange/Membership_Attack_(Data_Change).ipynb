{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Membership Attack - (Data Change).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amngb95/AI_CyberSecurity/blob/master/Lab2/DatasetChange/Membership_Attack_(Data_Change).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvK6LaVhrfZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# required imports\n",
        "import sys \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O05dkTKAsEh-",
        "colab_type": "code",
        "outputId": "e054e749-a984-44f3-f2ce-550b717d093a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# mount the google drive to download the datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "project_path = '/content/drive/My Drive'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEW1mLverl7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create transforms to load the images, nothing much is needed here. \n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Normalize the test set same as training set without augmentation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjsFJTFwr1wV",
        "colab_type": "code",
        "outputId": "d3f1d3ba-5649-4c24-9c6c-3edbd0fc0fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# download CIFAR 10 training set\n",
        "trainset1 = torchvision.datasets.CIFAR10(root= project_path+'/data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "\n",
        "# load the trainning set\n",
        "trainloader1 = torch.utils.data.DataLoader(trainset1, batch_size=4, shuffle=True)\n",
        "\n",
        "# download the test data\n",
        "testset1 = torchvision.datasets.CIFAR10(root= project_path+'/data', train=False,\n",
        "                                        download=True, transform=transform_test)\n",
        "\n",
        "# load the test data\n",
        "testloader1 = torch.utils.data.DataLoader(testset1, batch_size=4, shuffle=True)\n",
        "\n",
        "classes1 = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "# check those manually on the dataset site: https://www.cs.toronto.edu/~kriz/cifar.html"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTPw8Oymtlol",
        "colab_type": "code",
        "outputId": "2a9d20a6-92ce-4963-e433-d99e522d648b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# download CIFAR 10 training set\n",
        "trainset2 = torchvision.datasets.STL10(root= project_path+'/data', \n",
        "                                        split='train' ,download=True, transform=transform_train)\n",
        "\n",
        "# load the trainning set\n",
        "trainloader2 = torch.utils.data.DataLoader(trainset2, batch_size=4, shuffle=True)\n",
        "\n",
        "# download the test data\n",
        "testset2 = torchvision.datasets.STL10(root= project_path+'/data', \n",
        "                                        split='test',download=True, transform=transform_test)\n",
        "\n",
        "# load the test data\n",
        "testloader2 = torch.utils.data.DataLoader(testset2, batch_size=4, shuffle=True)\n",
        "\n",
        "classes2 = ('whale', 'shark', 'roses', 'cans', 'oranges', 'lamp', 'table', 'butterfly', 'lion', 'house')\n",
        "# check those manually on the dataset site: https://www.cs.toronto.edu/~kriz/cifar.html"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itY7G_y3tDnC",
        "colab_type": "code",
        "outputId": "2d665eef-73ac-4f02-ba37-2a9cd7b4e9ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# helper function to unnormalize and plot image \n",
        "def imshow(img):\n",
        "    img = np.array(img)\n",
        "    img = img / 2 + 0.5\n",
        "    img = np.moveaxis(img, 0, -1)\n",
        "    plt.imshow(img)\n",
        "    \n",
        "# display sample from dataset \n",
        "imgs, labels = iter(trainloader1).next()\n",
        "imshow(torchvision.utils.make_grid(imgs)) \n",
        "\n",
        "# notice who we converted the class idx to labels\n",
        "print(' '.join('%5s' % classes1[labels[j]] for j in range(4)))\n",
        "\n",
        "# run this cell multiple times and notice diff images\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  dog   car horse  bird\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3dYWxJJvneO",
        "colab_type": "code",
        "outputId": "63d18c9b-49cb-4c99-c424-2303b14dda39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# display sample from dataset \n",
        "imgs, labels = iter(trainloader2).next()\n",
        "imshow(torchvision.utils.make_grid(imgs)) \n",
        "\n",
        "# notice who we converted the class idx to labels\n",
        "print(' '.join('%5s' % classes2[labels[j]] for j in range(4)))\n",
        "\n",
        "# run this cell multiple times and notice diff images"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " lamp whale whale house\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6IIgCHXC/weaRnFlxkwplgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJK32k42gbEK",
        "colab_type": "code",
        "outputId": "50bd6cf9-7e02-4642-88bb-df233f64a80a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# divide the training dataset into the required groups Make sure they are balanced\n",
        "# original trainset is made of 50k images\n",
        "\n",
        "total_size1 = len(trainset1)\n",
        "split1a = total_size1 // 4\n",
        "split2a = split1a * 2\n",
        "split3a = split1a * 3\n",
        "\n",
        "print(total_size1, split1a, split2a, split3a)\n",
        "\n",
        "indices1 = list(range(total_size1))\n",
        "\n",
        "total_size2 = len(trainset2)\n",
        "split1b = total_size2 // 4\n",
        "split2b = split1b * 2\n",
        "split3b = split1b * 3\n",
        "\n",
        "print(total_size2, split1b, split2b, split3b)\n",
        "\n",
        "indices2 = list(range(total_size2))\n",
        "\n",
        "# two groups to train the shadow (in and out)\n",
        "shadow_train_idx = indices1[:split1a]\n",
        "shadow_out_idx = indices1[split1a:split2a]\n",
        "\n",
        "# two groups to train the Target (in and out)\n",
        "target_train_idx = indices2[split2b:split3b]\n",
        "target_out_idx =  indices2[split3b:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 12500 25000 37500\n",
            "5000 1250 2500 3750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4wJ_0lkhp76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16# pick your own\n",
        "\n",
        "# divide and load shadow train in and out\n",
        "shadow_train_sampler = SubsetRandomSampler(shadow_train_idx) # Pytorch function\n",
        "shadow_train_loader = torch.utils.data.DataLoader(trainset1, batch_size=batch_size, sampler=shadow_train_sampler)\n",
        "\n",
        "shadow_out_sampler = SubsetRandomSampler(shadow_out_idx) # Pytorch function\n",
        "shadow_out_loader = torch.utils.data.DataLoader(trainset1, batch_size=batch_size, sampler=shadow_out_sampler)\n",
        "\n",
        "# divide and load Target in and out\n",
        "target_train_sampler = SubsetRandomSampler(target_train_idx) # Pytorch function\n",
        "target_train_loader = torch.utils.data.DataLoader(trainset2, batch_size=batch_size, sampler=target_train_sampler)\n",
        "\n",
        "target_out_sampler = SubsetRandomSampler(target_out_idx) # Pytorch function\n",
        "target_out_loader = torch.utils.data.DataLoader(trainset2, batch_size=batch_size, sampler=target_out_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj9LuQJNuCXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "# create a CNN\n",
        "# Input shape (3, 32, 32) \n",
        "# architecture: simple. 2 conv and 2 Max pool, followed by 2 fc (120, 84) \n",
        "# output of fc is 10 because we have 10 classes!\n",
        "\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"CNN.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"CNN Builder.\"\"\"\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv_layer = nn.Sequential(\n",
        "\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout2d(p=0.05),\n",
        "\n",
        "            # Conv Layer block 3\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(4096, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform forward.\"\"\"\n",
        "        \n",
        "        # conv layers\n",
        "        x = self.conv_layer(x)\n",
        "        \n",
        "        # flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # fc layer\n",
        "        x = self.fc_layer(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O39H-xzClPgh",
        "colab_type": "code",
        "outputId": "ec0c789a-5ee7-4724-a11a-804f89374f4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# check if CUDA available or not\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "# clear the cache\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER_B4V8YjKyU",
        "colab_type": "code",
        "outputId": "bf33ee08-b974-4e00-fe36-df393b47c978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# initalize a target model and train it\n",
        "\n",
        "target_model = CNN()\n",
        "taget_model = target_model.cuda()\n",
        "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss\n",
        "optimizer = optim.Adam(target_model.parameters(), lr=0.0003) # try Adam VS SGD\n",
        "\n",
        "    \n",
        "epochs = 20\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in target_train_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        logits = target_model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(\"\\nEpoch : {}/{}..\".format(e+1,epochs),f\"Training loss: {running_loss/len(target_train_loader)}\")\n",
        "\n",
        "#save the model\n",
        "print(\"Model: \\n\\n\", target_model, '\\n')\n",
        "torch.save(target_model.state_dict(), project_path+'/target_checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch : 1/20.. Training loss: 2.229173169860357\n",
            "\n",
            "Epoch : 2/20.. Training loss: 2.1305382840241056\n",
            "\n",
            "Epoch : 3/20.. Training loss: 2.106651988210557\n",
            "\n",
            "Epoch : 4/20.. Training loss: 2.0225892791265174\n",
            "\n",
            "Epoch : 5/20.. Training loss: 2.008793268022658\n",
            "\n",
            "Epoch : 6/20.. Training loss: 1.9845786079575745\n",
            "\n",
            "Epoch : 7/20.. Training loss: 1.972193462939202\n",
            "\n",
            "Epoch : 8/20.. Training loss: 1.9278805859481232\n",
            "\n",
            "Epoch : 9/20.. Training loss: 1.9439607707760003\n",
            "\n",
            "Epoch : 10/20.. Training loss: 1.9099413231958318\n",
            "\n",
            "Epoch : 11/20.. Training loss: 1.9222618296176572\n",
            "\n",
            "Epoch : 12/20.. Training loss: 1.9206467821628233\n",
            "\n",
            "Epoch : 13/20.. Training loss: 1.8822112083435059\n",
            "\n",
            "Epoch : 14/20.. Training loss: 1.8979822397232056\n",
            "\n",
            "Epoch : 15/20.. Training loss: 1.9032473473609248\n",
            "\n",
            "Epoch : 16/20.. Training loss: 1.9174515959582752\n",
            "\n",
            "Epoch : 17/20.. Training loss: 1.8486856493768813\n",
            "\n",
            "Epoch : 18/20.. Training loss: 1.8736573035203958\n",
            "\n",
            "Epoch : 19/20.. Training loss: 1.859326011017908\n",
            "\n",
            "Epoch : 20/20.. Training loss: 1.865379295771635\n",
            "Model: \n",
            "\n",
            " CNN(\n",
            "  (conv_layer): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU(inplace)\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace)\n",
            "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): ReLU(inplace)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): Dropout2d(p=0.05)\n",
            "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (15): ReLU(inplace)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc_layer): Sequential(\n",
            "    (0): Dropout(p=0.1)\n",
            "    (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (4): ReLU(inplace)\n",
            "    (5): Dropout(p=0.1)\n",
            "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (7): LogSoftmax()\n",
            "  )\n",
            ") \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpY8ktdskRQN",
        "colab_type": "code",
        "outputId": "2bf0f88a-ae6c-42d6-eccd-2edb9962788a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# calculate the accuracy of the Target Model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in target_out_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = target_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 12500 test images (Cifar10 data): %d %%' % (100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 12500 test images (Cifar10 data): 24 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEClKFqikmUl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha_VqRVVkoCm",
        "colab_type": "code",
        "outputId": "deb23a6c-957e-4ee3-917f-1389a81c3c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# initalize a Shadow Model and Train it\n",
        "# for the first ICP, your shadow model can have the same CNN architecture and hyperparameters\n",
        "\n",
        "shadow_model = CNN()\n",
        "# clear the cache\n",
        "torch.cuda.empty_cache()\n",
        "#send to GPU\n",
        "shadow_model = shadow_model.cuda()\n",
        "shadow_criterion =  nn.CrossEntropyLoss() # CrossEntropyLoss\n",
        "shadow_optimizer = optim.Adam(shadow_model.parameters(), lr=0.0003) # ADAM \n",
        "\n",
        "\n",
        "# let the magic begin\n",
        "epochs = 20\n",
        "with torch.set_grad_enabled(True):\n",
        "  for e in range(epochs):\n",
        "      running_loss = 0\n",
        "      for images, labels in shadow_train_loader:\n",
        "          # sending tensors to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "          shadow_optimizer.zero_grad()\n",
        "          logits = shadow_model(images)\n",
        "          shadow_loss = shadow_criterion(logits, labels)\n",
        "          shadow_loss.backward()\n",
        "          shadow_optimizer.step()\n",
        "\n",
        "\n",
        "          running_loss += shadow_loss.item()\n",
        "      else:\n",
        "          print(\"\\nEpoch : {}/{}..\".format(e+1,epochs),f\"Training loss: {running_loss/len(shadow_train_loader)}\")\n",
        "\n",
        "#save the model\n",
        "print(\"Our model: \\n\\n\", shadow_model, '\\n')\n",
        "torch.save(shadow_model.state_dict(), project_path+'/shadow_checkpoint.pth')\n",
        "print('Finished Training the Shadow model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch : 1/20.. Training loss: 1.7904457638940543\n",
            "\n",
            "Epoch : 2/20.. Training loss: 1.4927572009661008\n",
            "\n",
            "Epoch : 3/20.. Training loss: 1.291610455269094\n",
            "\n",
            "Epoch : 4/20.. Training loss: 1.1476264825028837\n",
            "\n",
            "Epoch : 5/20.. Training loss: 1.0468432715786693\n",
            "\n",
            "Epoch : 6/20.. Training loss: 0.9706698018495384\n",
            "\n",
            "Epoch : 7/20.. Training loss: 0.8879454076061468\n",
            "\n",
            "Epoch : 8/20.. Training loss: 0.8272526573075358\n",
            "\n",
            "Epoch : 9/20.. Training loss: 0.7750277130500131\n",
            "\n",
            "Epoch : 10/20.. Training loss: 0.7319775208297288\n",
            "\n",
            "Epoch : 11/20.. Training loss: 0.6785643851703695\n",
            "\n",
            "Epoch : 12/20.. Training loss: 0.6546884928841877\n",
            "\n",
            "Epoch : 13/20.. Training loss: 0.626808594502604\n",
            "\n",
            "Epoch : 14/20.. Training loss: 0.6039349687907397\n",
            "\n",
            "Epoch : 15/20.. Training loss: 0.5680619002512807\n",
            "\n",
            "Epoch : 16/20.. Training loss: 0.5446321343290893\n",
            "\n",
            "Epoch : 17/20.. Training loss: 0.5069927369218196\n",
            "\n",
            "Epoch : 18/20.. Training loss: 0.49844621229783426\n",
            "\n",
            "Epoch : 19/20.. Training loss: 0.4710648829912972\n",
            "\n",
            "Epoch : 20/20.. Training loss: 0.44965174294474636\n",
            "Our model: \n",
            "\n",
            " CNN(\n",
            "  (conv_layer): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU(inplace)\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace)\n",
            "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): ReLU(inplace)\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): Dropout2d(p=0.05)\n",
            "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (15): ReLU(inplace)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc_layer): Sequential(\n",
            "    (0): Dropout(p=0.1)\n",
            "    (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (4): ReLU(inplace)\n",
            "    (5): Dropout(p=0.1)\n",
            "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (7): LogSoftmax()\n",
            "  )\n",
            ") \n",
            "\n",
            "Finished Training the Shadow model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0kP_-62ljFE",
        "colab_type": "code",
        "outputId": "88dea4bf-4387-4776-c2e2-e8526a428366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# calculate the accuracy of the Shadow Model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in shadow_out_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = shadow_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        #print(predicted)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 1250 test images (STL10 data): %d %%' % (100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 1250 test images (STL10 data): 76 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ymKnj7QpdDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = CNN()\n",
        "    model.load_state_dict(checkpoint)\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J1B1OhMp6er",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 1# pick your own\n",
        "\n",
        "# divide and load shadow train in and out\n",
        "shadow_train_sampler = SubsetRandomSampler(shadow_train_idx) # Pytorch function\n",
        "shadow_train_loader = torch.utils.data.DataLoader(trainset1, batch_size=batch_size, sampler=shadow_train_sampler)\n",
        "\n",
        "shadow_out_sampler = SubsetRandomSampler(shadow_out_idx) # Pytorch function\n",
        "shadow_out_loader = torch.utils.data.DataLoader(trainset1, batch_size=batch_size, sampler=shadow_out_sampler)\n",
        "\n",
        "# divide and load Target in and out\n",
        "target_train_sampler = SubsetRandomSampler(target_train_idx) # Pytorch function\n",
        "target_train_loader = torch.utils.data.DataLoader(trainset2, batch_size=batch_size, sampler=target_train_sampler)\n",
        "\n",
        "target_out_sampler = SubsetRandomSampler(target_out_idx) # Pytorch function\n",
        "target_out_loader = torch.utils.data.DataLoader(trainset2, batch_size=batch_size, sampler=target_out_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaD8_N34l3Eh",
        "colab_type": "code",
        "outputId": "11a71822-7389-4f1c-8344-5c525e6b01ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#load the model\n",
        "load_shadow_model = load_checkpoint(project_path+'/shadow_checkpoint.pth')\n",
        "load_shadow_model = load_shadow_model.cuda()\n",
        "\n",
        "# freeze the Shadow model \n",
        "for param in load_shadow_model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "# make predictions on both datasets (shadow_in and shdow_out)\n",
        "predictions = []\n",
        "\n",
        "labels_0 = 0#np.zeros(1)\n",
        "labels_1 = 1#np.ones(1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in shadow_train_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        logps = load_shadow_model(images)\n",
        "        ps = torch.exp(logps) \n",
        "        ps = ps.cpu()\n",
        "        pred = ps.data.numpy()\n",
        "        predictions.append([pred[0],labels_1])   \n",
        "with torch.no_grad():\n",
        "    for images, labels in shadow_out_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        logps = load_shadow_model(images)\n",
        "        ps = torch.exp(logps) \n",
        "        ps = ps.cpu()\n",
        "        pred = ps.data.numpy()\n",
        "        predictions.append([pred[0],labels_0]) \n",
        "        \n",
        "print(predictions[0])  \n",
        "print(predictions[17000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([0.08596601, 0.04023437, 0.02021134, 0.01794253, 0.08859356,\n",
            "       0.03292565, 0.00500259, 0.5770346 , 0.00919348, 0.12289599],\n",
            "      dtype=float32), 1]\n",
            "[array([7.39586577e-02, 4.00119927e-04, 8.33559275e-01, 5.76939760e-03,\n",
            "       7.30520952e-03, 1.05175911e-03, 7.63668939e-02, 1.20160425e-04,\n",
            "       1.07317849e-03, 3.95484618e-04], dtype=float32), 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhaKjRRUl6Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a new dataset of the shape [predictions(shadow_in), 1], [predicitons(shadow_out), 0] and zip them together\n",
        "#save the dataset\n",
        "import pickle\n",
        "\n",
        "with open(project_path+'/data/shadow.data', 'wb') as filehandle:\n",
        "    pickle.dump(predictions, filehandle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl56BLIpl8Nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the recall and precision of your attack network using the Target_out and Target_in datasets\n",
        "# to do so, take a random numer of datapoints, run them throw the target model,\n",
        "#load the model\n",
        "load_target_model = load_checkpoint(project_path+'/target_checkpoint.pth')\n",
        "load_target_model = load_target_model.cuda()\n",
        "\n",
        "# freeze the Shadow model \n",
        "for param in load_target_model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "# make predictions on both datasets (shadow_in and shdow_out)\n",
        "predictions = []\n",
        "label_size = (1,1)\n",
        "\n",
        "labels_0 = 0\n",
        "labels_1 = 1\n",
        "with torch.no_grad():\n",
        "    for images, labels in target_train_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        logps = load_target_model(images)\n",
        "        ps = torch.exp(logps) \n",
        "        ps = ps.cpu()\n",
        "        pred = ps.data.numpy()\n",
        "        predictions.append([pred[0],labels_1])   \n",
        "with torch.no_grad():\n",
        "    for images, labels in target_out_loader:\n",
        "        # sending tensors to GPU\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        logps = load_target_model(images)\n",
        "        ps = torch.exp(logps) \n",
        "        ps = ps.cpu()\n",
        "        pred = ps.data.numpy()\n",
        "        predictions.append([pred[0],labels_0]) \n",
        "        \n",
        "#save the dataset\n",
        "import pickle\n",
        "\n",
        "with open(project_path+'/data/target.data', 'wb') as filehandle:\n",
        "    pickle.dump(predictions, filehandle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yEnRcZW6sUY",
        "colab_type": "code",
        "outputId": "b7d2ee1c-7db0-4517-b9f7-7f2773fbfe55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# check if CUDA available or not\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "# clear the cache\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UZ2tfJzl94s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the dataset\n",
        "with open(project_path+'/data/shadow.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    predictionsList = pickle.load(filehandle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJFbXp8umAam",
        "colab_type": "code",
        "outputId": "cc4740cb-b798-427c-adbf-df2e43bc8d2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "total_size = len(predictionsList)\n",
        "split1 = total_size // 4\n",
        "split1 = total_size - split1 \n",
        "split2 = split1*2\n",
        "indices = list(range(total_size))\n",
        "\n",
        "# two groups to train the shadow (in and out)\n",
        "train_idx = indices[:] \n",
        "test_idx = indices[split1:] \n",
        "print(f'No.of train date {len(train_idx)} and No.of test data {len(test_idx)}')\n",
        "batch_size = 10 # pick your own\n",
        "\n",
        "# divide and load shadow train in and out\n",
        "train_sampler = SubsetRandomSampler(train_idx) # Pytorch function\n",
        "train_loader = torch.utils.data.DataLoader(predictionsList, batch_size=batch_size, sampler=train_sampler)\n",
        "\n",
        "test_sampler = SubsetRandomSampler(test_idx) # Pytorch function\n",
        "test_loader = torch.utils.data.DataLoader(predictionsList, batch_size=batch_size, sampler=test_sampler)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No.of train date 25000 and No.of test data 6250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU012hhrmDvi",
        "colab_type": "code",
        "outputId": "4311a087-206b-499e-9cc3-964dd29a62dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# create the Attack Model: A NN binary classifier {0, 1}\n",
        "# the input to this model is the propability distribution vector of size 10\n",
        "# and the output is either 0 (input was not included in training) or 1\n",
        "from torch.autograd import Variable\n",
        "attack_model = nn.Sequential(nn.Linear(10, 20),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(20, 26),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(26, 16),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(16, 8),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(8, 2),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "attack_model = attack_model.cuda()\n",
        "attack_criterion = nn.CrossEntropyLoss()\n",
        "attack_optimizer = optim.Adam(attack_model.parameters(), lr=0.003)\n",
        "\n",
        "epochs = 5\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for outputs, labels in train_loader:\n",
        "        # sending tensors to GPU\n",
        "        outputs = outputs.cuda().float()\n",
        "        labels = labels.cuda().long()\n",
        "        attack_optimizer.zero_grad()\n",
        "        pred = torch.exp(attack_model(outputs))\n",
        "        #print(pred.data, labels)\n",
        "        loss = attack_criterion(pred, labels)\n",
        "        loss.backward()\n",
        "        attack_optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(f\"Training loss: {running_loss/len(predictionsList)}\")\n",
        "        \n",
        "torch.save(attack_model.state_dict(), project_path+'/attack_checkpoint.pth')\n",
        "print('Finished Training the Attack model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss: 0.06935383262634277\n",
            "Training loss: 0.06934928565263748\n",
            "Training loss: 0.06932225021362305\n",
            "Training loss: 0.06931861157894134\n",
            "Training loss: 0.06930207609653473\n",
            "Finished Training the Attack model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWR_CXQEmIC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the dataset\n",
        "with open(project_path+'/data/target.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    validation = pickle.load(filehandle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecc2gaFmHnoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 1 # pick your own\n",
        "\n",
        "validation_sampler = SubsetRandomSampler(indices[500:2500]) # randomly picking 5000 data items\n",
        "validation_loader = torch.utils.data.DataLoader(validation, batch_size=batch_size, sampler=validation_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haNVIg-FHsFv",
        "colab_type": "code",
        "outputId": "71735a01-e338-4025-bc3a-e310703ed1d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "#input the output of the target model to your attack network \n",
        "# you already know the target_in and target_out samples, so use that info to evaluate the attack model\n",
        "correct = 0\n",
        "incorrect = 0\n",
        "total = 0\n",
        "tp = 0\n",
        "tn = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "with torch.no_grad():\n",
        "    for outputs, labels in validation_loader:\n",
        "        # sending tensors to GPU\n",
        "        outputs = outputs.cuda().float()\n",
        "        labels = labels.cuda().long()\n",
        "        pred = torch.exp(attack_model(outputs))\n",
        "        predicted = torch.argmax(pred.data)\n",
        "        #print('\\n',pred.data, predicted)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted.item() == labels.item())\n",
        "        tp += ((predicted.item() == labels.item()) and (predicted.item() == 1))\n",
        "        tn += ((predicted.item() == labels.item()) and (predicted.item() == 0))\n",
        "        fp += ((predicted.item() != labels.item()) and (predicted.item() == 1))\n",
        "        fn += ((predicted.item() != labels.item()) and (predicted.item() == 0))\n",
        "incorrect = total- correct\n",
        "print(f'TP : {tp}, TN : {tn}, FP : {fp}, FN : {fn}')\n",
        "pre = tp/(tp+fp)\n",
        "rec = tp/(tp+fn)\n",
        "print(f'Precision {pre*100}')\n",
        "print(f'Recall {rec*100}')\n",
        "print(f'F1 Score {2*((pre*rec)/(pre+rec))*100}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TP : 750, TN : 0, FP : 1250, FN : 0\n",
            "Precision 37.5\n",
            "Recall 100.0\n",
            "F1 Score 54.54545454545454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMsyCgziqeaa",
        "colab_type": "text"
      },
      "source": [
        "Great! At this point, you must have created a succesfful attack model that can detect whether a datapoint was used in training a target mode or not. \n",
        "* A successful attack model is one with a precision/recall higher than 85% -- you are using same architecture and are aware of the data classes\n",
        "\n",
        " \n",
        " Can you suggest any defense mechanism? If yes, Apply them to your solution and re-evaluate your attack model. How did your defense mecanism affect the accuracy of the target model? How did it affect the recall and precision of the Attack model?"
      ]
    }
  ]
}